{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pillow>=7.0.0 (from -r requirements.txt (line 1))\n",
      "  Obtaining dependency information for pillow>=7.0.0 from https://files.pythonhosted.org/packages/b7/ad/71982d18fd28ed1f93c31b8648f980ebdbdbcf7d8c9c9b4af59290914ce9/Pillow-10.0.0-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading Pillow-10.0.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (9.5 kB)\n",
      "Collecting requests>=2.20.0 (from -r requirements.txt (line 2))\n",
      "  Obtaining dependency information for requests>=2.20.0 from https://files.pythonhosted.org/packages/70/8e/0e2d847013cb52cd35b38c009bb167a1a26b2ce6cd6965bf26b47bc0bf44/requests-2.31.0-py3-none-any.whl.metadata\n",
      "  Downloading requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting opencv-python>=4.2.0.32 (from -r requirements.txt (line 3))\n",
      "  Obtaining dependency information for opencv-python>=4.2.0.32 from https://files.pythonhosted.org/packages/32/a6/4321f0f30ee11d6d85f49251d417f4e885fe7638b5ac50b7e3c80cccf141/opencv_python-4.8.0.76-cp37-abi3-macosx_11_0_arm64.whl.metadata\n",
      "  Using cached opencv_python-4.8.0.76-cp37-abi3-macosx_11_0_arm64.whl.metadata (19 kB)\n",
      "Collecting tqdm>=4.23.0 (from -r requirements.txt (line 4))\n",
      "  Obtaining dependency information for tqdm>=4.23.0 from https://files.pythonhosted.org/packages/00/e5/f12a80907d0884e6dff9c16d0c0114d81b8cd07dc3ae54c5e962cc83037e/tqdm-4.66.1-py3-none-any.whl.metadata\n",
      "  Downloading tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting diffimg==0.2.3 (from -r requirements.txt (line 5))\n",
      "  Using cached diffimg-0.2.3.tar.gz (4.1 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting arabic-reshaper==2.1.3 (from -r requirements.txt (line 6))\n",
      "  Using cached arabic_reshaper-2.1.3-py3-none-any.whl (20 kB)\n",
      "Collecting python-bidi==0.4.2 (from -r requirements.txt (line 7))\n",
      "  Using cached python_bidi-0.4.2-py2.py3-none-any.whl (30 kB)\n",
      "Collecting wikipedia>=1.4.0 (from -r requirements.txt (line 8))\n",
      "  Using cached wikipedia-1.4.0.tar.gz (27 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting future (from arabic-reshaper==2.1.3->-r requirements.txt (line 6))\n",
      "  Downloading future-0.18.3.tar.gz (840 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m840.9/840.9 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from arabic-reshaper==2.1.3->-r requirements.txt (line 6)) (65.5.0)\n",
      "Requirement already satisfied: six in /Users/IsRealNPK/Library/Python/3.11/lib/python/site-packages (from python-bidi==0.4.2->-r requirements.txt (line 7)) (1.16.0)\n",
      "Collecting charset-normalizer<4,>=2 (from requests>=2.20.0->-r requirements.txt (line 2))\n",
      "  Obtaining dependency information for charset-normalizer<4,>=2 from https://files.pythonhosted.org/packages/91/e6/8fa919fc84a106e9b04109de62bdf8526899e2754a64da66e1cd50ac1faa/charset_normalizer-3.2.0-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading charset_normalizer-3.2.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (31 kB)\n",
      "Collecting idna<4,>=2.5 (from requests>=2.20.0->-r requirements.txt (line 2))\n",
      "  Using cached idna-3.4-py3-none-any.whl (61 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.20.0->-r requirements.txt (line 2))\n",
      "  Obtaining dependency information for urllib3<3,>=1.21.1 from https://files.pythonhosted.org/packages/9b/81/62fd61001fa4b9d0df6e31d47ff49cfa9de4af03adecf339c7bc30656b37/urllib3-2.0.4-py3-none-any.whl.metadata\n",
      "  Downloading urllib3-2.0.4-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests>=2.20.0->-r requirements.txt (line 2))\n",
      "  Obtaining dependency information for certifi>=2017.4.17 from https://files.pythonhosted.org/packages/4c/dd/2234eab22353ffc7d94e8d13177aaa050113286e93e7b40eae01fbf7c3d9/certifi-2023.7.22-py3-none-any.whl.metadata\n",
      "  Downloading certifi-2023.7.22-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting numpy>=1.21.2 (from opencv-python>=4.2.0.32->-r requirements.txt (line 3))\n",
      "  Obtaining dependency information for numpy>=1.21.2 from https://files.pythonhosted.org/packages/86/a1/b8ef999c32f26a97b5f714887e21f96c12ae99a38583a0a96e65283ac0a1/numpy-1.25.2-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading numpy-1.25.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (5.6 kB)\n",
      "Collecting beautifulsoup4 (from wikipedia>=1.4.0->-r requirements.txt (line 8))\n",
      "  Downloading beautifulsoup4-4.12.2-py3-none-any.whl (142 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.0/143.0 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting soupsieve>1.2 (from beautifulsoup4->wikipedia>=1.4.0->-r requirements.txt (line 8))\n",
      "  Downloading soupsieve-2.4.1-py3-none-any.whl (36 kB)\n",
      "\u001b[33mWARNING: The candidate selected for download or install is a yanked version: 'arabic-reshaper' candidate (version 2.1.3 at https://files.pythonhosted.org/packages/47/27/7b9b824f5342d8ee180027333f2e15842ea36f5bc2d3d24a4e6bb31fb596/arabic_reshaper-2.1.3-py3-none-any.whl (from https://pypi.org/simple/arabic-reshaper/))\n",
      "Reason for being yanked: Doesn't work with Python 2\u001b[0m\u001b[33m\n",
      "\u001b[0mDownloading Pillow-10.0.0-cp311-cp311-macosx_11_0_arm64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.6/62.6 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached opencv_python-4.8.0.76-cp37-abi3-macosx_11_0_arm64.whl (33.1 MB)\n",
      "Downloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading certifi-2023.7.22-py3-none-any.whl (158 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.3/158.3 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading charset_normalizer-3.2.0-cp311-cp311-macosx_11_0_arm64.whl (122 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.8/122.8 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.25.2-cp311-cp311-macosx_11_0_arm64.whl (14.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading urllib3-2.0.4-py3-none-any.whl (123 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.9/123.9 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: diffimg, wikipedia, future\n",
      "  Building wheel for diffimg (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for diffimg: filename=diffimg-0.2.3-py3-none-any.whl size=4019 sha256=0804fb41b9c884fab2c0ceaa6089486482dc09a41a1c727f0eac34c9970e7704\n",
      "  Stored in directory: /Users/IsRealNPK/Library/Caches/pip/wheels/5c/29/02/db70c8f86dbb907e99b14d44e83644fdde3e29fc8101840480\n",
      "  Building wheel for wikipedia (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11678 sha256=331aaf183e203ffe2d64eae8b870ddfd05004956ec3ce7bf1a33f1d8b4f7b09c\n",
      "  Stored in directory: /Users/IsRealNPK/Library/Caches/pip/wheels/8f/ab/cb/45ccc40522d3a1c41e1d2ad53b8f33a62f394011ec38cd71c6\n",
      "  Building wheel for future (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for future: filename=future-0.18.3-py3-none-any.whl size=492024 sha256=46a19cc6df8108712e57a0a280a4c0fc14c9c366f16037d13809b6694695d83d\n",
      "  Stored in directory: /Users/IsRealNPK/Library/Caches/pip/wheels/da/19/ca/9d8c44cd311a955509d7e13da3f0bea42400c469ef825b580b\n",
      "Successfully built diffimg wikipedia future\n",
      "Installing collected packages: urllib3, tqdm, soupsieve, python-bidi, pillow, numpy, idna, future, charset-normalizer, certifi, requests, opencv-python, diffimg, beautifulsoup4, arabic-reshaper, wikipedia\n",
      "Successfully installed arabic-reshaper-2.1.3 beautifulsoup4-4.12.2 certifi-2023.7.22 charset-normalizer-3.2.0 diffimg-0.2.3 future-0.18.3 idna-3.4 numpy-1.25.2 opencv-python-4.8.0.76 pillow-10.0.0 python-bidi-0.4.2 requests-2.31.0 soupsieve-2.4.1 tqdm-4.66.1 urllib3-2.0.4 wikipedia-1.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('GPTNameTHEN.txt', sep=\".\", header=None, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>สมชาย ใจดี (Somchai Jai Dee)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>รุ่งทิวา สวยงาม (Rungtiwa Suayngam)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ภัทราพร รักชาติ (Patharaporn Rukchat)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>อรวรรณ มีรักษ์ (Orawan Mee-rak)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ณัฐวรรณ สุขใจ (Nattawan Sukjai)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Name\n",
       "0                                        \n",
       "1            สมชาย ใจดี (Somchai Jai Dee)\n",
       "2     รุ่งทิวา สวยงาม (Rungtiwa Suayngam)\n",
       "3   ภัทราพร รักชาติ (Patharaporn Rukchat)\n",
       "4         อรวรรณ มีรักษ์ (Orawan Mee-rak)\n",
       "5         ณัฐวรรณ สุขใจ (Nattawan Sukjai)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns = [\"Name\"]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "def en_th_sep(name = 'name'):\n",
    "    name = name.split('(')\n",
    "    name[1] = name[1].replace(')', '')\n",
    "    name[0] = name[0].strip()\n",
    "    if re.search(r'[ก-๙]', name[0]):\n",
    "        return name[1], name[0]\n",
    "    else:\n",
    "        return name[0], name[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['en'], df['th'] = zip(*df['Name'].map(en_th_sep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en</th>\n",
       "      <th>th</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Somchai Jai Dee</td>\n",
       "      <td>สมชาย ใจดี</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Rungtiwa Suayngam</td>\n",
       "      <td>รุ่งทิวา สวยงาม</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Patharaporn Rukchat</td>\n",
       "      <td>ภัทราพร รักชาติ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Orawan Mee-rak</td>\n",
       "      <td>อรวรรณ มีรักษ์</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Nattawan Sukjai</td>\n",
       "      <td>ณัฐวรรณ สุขใจ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    en               th\n",
       "0                                      \n",
       "1      Somchai Jai Dee       สมชาย ใจดี\n",
       "2    Rungtiwa Suayngam  รุ่งทิวา สวยงาม\n",
       "3  Patharaporn Rukchat  ภัทราพร รักชาติ\n",
       "4       Orawan Mee-rak   อรวรรณ มีรักษ์\n",
       "5      Nattawan Sukjai    ณัฐวรรณ สุขใจ"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop(columns=['Name'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trdg.utils import mask_to_bboxes, draw_bounding_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trdg.generators import (\n",
    "    GeneratorFromStrings )\n",
    "\n",
    "# The generators use the same arguments as the CLI, only as parameters\n",
    "generator = GeneratorFromStrings(\n",
    "    df['th'].tolist() + df['en'].tolist(),\n",
    "    # ['พ่อ', 'ลุง', 'ฉัน', 'ญู', 'นั่น', 'น่ัน', 'พี่', 'พ่ี', 'นู้น', 'นั้น', 'น้ำ', 'นำ้', 'ปำ', 'ฝ่ำ'],\n",
    "    blur=0,\n",
    "    random_blur=False,\n",
    "    language='th',\n",
    "    count=len(df['th'].tolist())*2,\n",
    "    # count=14,\n",
    "    fonts=[\n",
    "        'trdg/fonts/th/upcdb.ttf', \n",
    "        # 'trdg/fonts/th/upcdbi.ttf',\n",
    "        # 'trdg/fonts/th/upcdi.ttf', \n",
    "        'trdg/fonts/th/upcdl.ttf',\n",
    "    ],\n",
    "    size=64,\n",
    "    background_type=3,\n",
    "    image_dir='thai_id_bg/',\n",
    "    margins = (5,5,5,5),\n",
    "    output_mask=True,\n",
    "    output_bboxes=3,\n",
    ")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trdg.computer_text_generator import TH_TONE_MARKS, TH_UNDER_VOWELS, TH_UPPER_VOWELS, TH_SARA_AM\n",
    "from trdg.computer_text_generator import th_grouping\n",
    "def th_tone_vowel_bbox_sep(lbl, bboxes):\n",
    "    splitted_text = th_grouping(lbl)\n",
    "    # print(splitted_text)\n",
    "    # print(bboxes)\n",
    "    new_bboxes = []\n",
    "    for i, s in enumerate(splitted_text):\n",
    "        bbox = bboxes[i]\n",
    "        len_s = len(s)\n",
    "        if len_s > 1 and s != 'ญู' and s != 'ญุ':\n",
    "            if len_s == 2 :\n",
    "                s2 = \"{0:#x}\".format(ord(s[1]))\n",
    "                if s2 in TH_TONE_MARKS: # tone only\n",
    "                    h = bbox[3] - bbox[1]\n",
    "                    tone_bbox = (bbox[0], bbox[1], bbox[2], bbox[1]+round(h*0.3))\n",
    "                    new_bboxes.append(tone_bbox) \n",
    "                    alpha_bbox = (bbox[0], bbox[1] + round(h*0.3), bbox[2], bbox[3])\n",
    "                    new_bboxes.append(alpha_bbox)\n",
    "                elif s2 in TH_UPPER_VOWELS: # upper_vowel only\n",
    "                    h = bbox[3] - bbox[1]\n",
    "                    upper_vowel_bbox = (bbox[0], bbox[1], bbox[2], bbox[1]+round(h*0.3))\n",
    "                    new_bboxes.append(upper_vowel_bbox)\n",
    "                    alpha_bbox = (bbox[0], bbox[1] + round(h*0.3), bbox[2], bbox[3])\n",
    "                    new_bboxes.append(alpha_bbox)\n",
    "                elif s2 in TH_UNDER_VOWELS: # under_vowel only\n",
    "                    h = bbox[3] - bbox[1]\n",
    "                    alpha_bbox = (bbox[0], bbox[1], bbox[2], bbox[1]+round(h*0.7))\n",
    "                    new_bboxes.append(alpha_bbox)\n",
    "                    under_vowel_bbox = (bbox[0], bbox[1]+round(h*0.7), bbox[2], bbox[3])\n",
    "                    new_bboxes.append(under_vowel_bbox)\n",
    "                elif s2 in TH_SARA_AM: # sara_am only\n",
    "                    if s[0] in ['ป', 'ฝ']:\n",
    "                        w = bbox[2] - bbox[0]\n",
    "                        alpha_bbox = (bbox[0], bbox[1], bbox[0]+round(w*0.6), bbox[3])\n",
    "                        new_bboxes.append(alpha_bbox)\n",
    "                        sara_am_bbox = (bbox[0]+round(w*0.2), bbox[1], bbox[2], bbox[3])\n",
    "                        new_bboxes.append(sara_am_bbox)\n",
    "                    else :\n",
    "                        w = bbox[2] - bbox[0]\n",
    "                        alpha_bbox = (bbox[0], bbox[1], bbox[0]+round(w*0.6), bbox[3])\n",
    "                        new_bboxes.append(alpha_bbox)\n",
    "                        sara_am_bbox = (bbox[0]+round(w*0.4), bbox[1], bbox[2], bbox[3])\n",
    "                        new_bboxes.append(sara_am_bbox)\n",
    "                    \n",
    "                else :\n",
    "                    new_bboxes.append(bbox)\n",
    "            elif len_s == 3 :\n",
    "                s2 = \"{0:#x}\".format(ord(s[1]))\n",
    "                s3 = \"{0:#x}\".format(ord(s[2]))\n",
    "                if s2 in TH_UPPER_VOWELS and s3 in TH_TONE_MARKS: # upper_vowel + tone\n",
    "                    h = bbox[3] - bbox[1]\n",
    "                    tone_bbox = (bbox[0], bbox[1], bbox[2], bbox[1]+round(h*0.23))\n",
    "                    new_bboxes.append(tone_bbox)\n",
    "                    upper_vowel_bbox = (bbox[0], bbox[1]+round(h*0.23), bbox[2], bbox[1]+round(h*0.5))\n",
    "                    new_bboxes.append(upper_vowel_bbox)\n",
    "                    alpha_bbox = (bbox[0], bbox[1]+round(h*0.5), bbox[2], bbox[3])\n",
    "                    new_bboxes.append(alpha_bbox)\n",
    "                \n",
    "                elif s2 in TH_UNDER_VOWELS and s3 in TH_TONE_MARKS: # under_vowel + tone    \n",
    "                    h = bbox[3] - bbox[1]\n",
    "                    tone_bbox = (bbox[0], bbox[1], bbox[2], bbox[1]+round(h*0.23))\n",
    "                    new_bboxes.append(tone_bbox)\n",
    "                    alpha_bbox = (bbox[0], bbox[1]+round(h*0.23), bbox[2], bbox[1]+round(h*0.72))\n",
    "                    new_bboxes.append(alpha_bbox)\n",
    "                    under_vowel_bbox = (bbox[0], bbox[1]+round(h*0.72), bbox[2], bbox[3])\n",
    "                    new_bboxes.append(under_vowel_bbox)\n",
    "                elif s2 in TH_TONE_MARKS and s3 in TH_SARA_AM: # sara_am + tone\n",
    "                    h = bbox[3] - bbox[1]\n",
    "                    tone_bbox = (bbox[0], bbox[1], bbox[2], bbox[1]+round(h*0.25))\n",
    "                    new_bboxes.append(tone_bbox)\n",
    "                    sara_am_bbox = (bbox[0], bbox[1]+round(h*0.3), bbox[2], bbox[3])\n",
    "                    if s[0] in ['ป', 'ฝ']:\n",
    "                        w = bbox[2] - bbox[0]\n",
    "                        alpha_bbox = (sara_am_bbox[0], sara_am_bbox[1], sara_am_bbox[0]+round(w*0.6), sara_am_bbox[3])\n",
    "                        new_bboxes.append(alpha_bbox)\n",
    "                        sara_am_bbox = (sara_am_bbox[0]+round(w*0.2), sara_am_bbox[1], sara_am_bbox[2], sara_am_bbox[3])\n",
    "                        new_bboxes.append(sara_am_bbox)\n",
    "                    else :\n",
    "                        w = bbox[2] - bbox[0]\n",
    "                        alpha_bbox = (sara_am_bbox[0], sara_am_bbox[1], sara_am_bbox[0]+round(w*0.6), sara_am_bbox[3])\n",
    "                        new_bboxes.append(alpha_bbox)\n",
    "                        sara_am_bbox = (sara_am_bbox[0]+round(w*0.4), sara_am_bbox[1], sara_am_bbox[2], sara_am_bbox[3])\n",
    "                        new_bboxes.append(sara_am_bbox)\n",
    "                else :\n",
    "                    new_bboxes.append(bbox)\n",
    "        else :\n",
    "            new_bboxes.append(bbox)\n",
    "    # print(new_bboxes)\n",
    "    return splitted_text, new_bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PIL import ImageDraw\n",
    "# for (img, mask), lbl in generator:\n",
    "#     bboxes = mask_to_bboxes(mask)\n",
    "#     lbl, bboxes = th_tone_vowel_bbox_sep(lbl, bboxes)\n",
    "#     lbl = ''.join(lbl)\n",
    "#     print(lbl)\n",
    "#     print(f'{len(bboxes)} bboxes : {len(lbl)} chars')\n",
    "#     d = ImageDraw.Draw(img)\n",
    "#     for bbox in bboxes:\n",
    "#         d.rectangle(bbox, outline=\"green\")\n",
    "#     display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [00:05, 39.40it/s]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from pyblp import save_pickle\n",
    "\n",
    "def current_milli_time():\n",
    "    return round(time.time() * 1000)\n",
    "\n",
    "project_dir = 'ThaiID'\n",
    "full_path =os.path.dirname(os.path.realpath(\"__file__\"))\n",
    "project_dir = os.path.join(full_path,project_dir)\n",
    "dirname = 'train'\n",
    "dir_path = os.path.join(project_dir,dirname)\n",
    "os.makedirs(dir_path,exist_ok=True)\n",
    "texts = []\n",
    "save_dir_img =os.path.join(dir_path,'imgs')\n",
    "# rmtree(save_dir_img,ignore_errors=True)\n",
    "os.makedirs(save_dir_img,exist_ok=True)\n",
    "\n",
    "save_dir_bbox =os.path.join(dir_path,'bbox')\n",
    "# rmtree(save_dir_bbox,ignore_errors=True)\n",
    "os.makedirs(save_dir_bbox,exist_ok=True)\n",
    "\n",
    "save_dir_mask =os.path.join(dir_path,'mask')\n",
    "# rmtree(save_dir_mask,ignore_errors=True)\n",
    "os.makedirs(save_dir_mask,exist_ok=True)\n",
    "\n",
    "for i in tqdm(generator):\n",
    "    if i[0]:\n",
    "        (img, mask), lbl = i\n",
    "        try:\n",
    "            # lbl = lbl.replace('—','')\n",
    "            # display(img)\n",
    "            bboxes = mask_to_bboxes(mask)\n",
    "            lbl, bboxes = th_tone_vowel_bbox_sep(lbl, bboxes)\n",
    "            lbl = ''.join(lbl)\n",
    "            timestamp = current_milli_time()\n",
    "            filename = f'{timestamp}.jpg'\n",
    "            filename_mask = f'{timestamp}.png'\n",
    "            bbox_filename = f'{timestamp}.pkl'\n",
    "            image_path = os.path.join(save_dir_img,filename)\n",
    "            bbox_path = os.path.join(save_dir_bbox,bbox_filename)\n",
    "\n",
    "            mask_path = os.path.join(save_dir_mask,filename_mask)\n",
    "            save_pickle(bboxes, bbox_path)\n",
    "            # txt_imagepath =os.path.join('test',filename)\n",
    "\n",
    "            # resized = img.resize((340,70))\n",
    "            img.save(image_path)\n",
    "            mask.save(mask_path)\n",
    "            texts.append(f'{filename} {lbl}\\n')\n",
    "        # print(lbl)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(lbl)\n",
    "with open(os.path.join(dir_path,'gt.txt'),'a') as f:\n",
    "    f.writelines(''.join(texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test with a single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=600x100>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from PIL import Image, ImageFont, ImageDraw\n",
    "\n",
    "text_string = str(['นั่น', 'น่ัน', 'พี่', 'พ่ี', 'น้ำ', 'นำ้'])\n",
    "\n",
    "img = Image.new('RGB', (600, 100))\n",
    "draw = ImageDraw.Draw(img)\n",
    "font = ImageFont.truetype('trdg/fonts/th/upcdb.ttf', 50, layout_engine=ImageFont.Layout.RAQM)\n",
    "\n",
    "draw.text((25,40), text_string, fill='white', font=font)\n",
    "\n",
    "display(img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
